{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Checking paths:\n",
      "Project Root: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT\n",
      "Exists: True\n",
      "Input Directory: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT/input/ranzcr-clip-catheter-line-classification\n",
      "Exists: True\n",
      "Train Path: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT/input/ranzcr-clip-catheter-line-classification/train\n",
      "Exists: True\n",
      "Test Path: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT/input/ranzcr-clip-catheter-line-classification/test\n",
      "Exists: True\n",
      "Output Directory: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT/notebooks/output\n",
      "Exists: True\n",
      "\n",
      "First few training images:\n",
      "Image exists: 1.2.826.0.1.3680043.8.498.16451034714945708059993280774682419855.jpg - True\n",
      "Image exists: 1.2.826.0.1.3680043.8.498.20326719114358003969350032771972492089.jpg - True\n",
      "Image exists: 1.2.826.0.1.3680043.8.498.14437343951736802599696198383777590245.jpg - True\n",
      "Image exists: 1.2.826.0.1.3680043.8.498.76963676309112173543961651520072177748.jpg - True\n",
      "Image exists: 1.2.826.0.1.3680043.8.498.59684212151785159760704246549335054351.jpg - True\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop,\n",
    "    HorizontalFlip, VerticalFlip, RandomBrightnessContrast,\n",
    "    Rotate, ShiftScaleRotate, CoarseDropout, Transpose  # Changed Cutout to CoarseDropout\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check available device\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "NOTEBOOK_DIR = Path(os.getcwd())  \n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent  \n",
    "\n",
    "\n",
    "INPUT_DIR = PROJECT_ROOT / 'input' / 'ranzcr-clip-catheter-line-classification'\n",
    "TRAIN_PATH = INPUT_DIR / 'train'\n",
    "TEST_PATH = INPUT_DIR / 'test'\n",
    "OUTPUT_DIR = NOTEBOOK_DIR / 'output' \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def verify_paths():\n",
    "    paths = {\n",
    "        'Project Root': PROJECT_ROOT,\n",
    "        'Input Directory': INPUT_DIR,\n",
    "        'Train Path': TRAIN_PATH,\n",
    "        'Test Path': TEST_PATH,\n",
    "        'Output Directory': OUTPUT_DIR\n",
    "    }\n",
    "    \n",
    "    print(\"Checking paths:\")\n",
    "    for name, path in paths.items():\n",
    "        exists = path.exists()\n",
    "        print(f\"{name}: {path}\")\n",
    "        print(f\"Exists: {exists}\")\n",
    "        if not exists:\n",
    "            print(f\"WARNING: {name} does not exist!\")\n",
    "    \n",
    "    \n",
    "    if TRAIN_PATH.exists():\n",
    "        train_images = list(TRAIN_PATH.glob('*.jpg'))[:5]  \n",
    "        print(f\"\\nFirst few training images:\")\n",
    "        for img_path in train_images:\n",
    "            print(f\"Image exists: {img_path.name} - {img_path.exists()}\")\n",
    "\n",
    "\n",
    "verify_paths()\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Config\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    print_freq = 100\n",
    "    num_workers = 0\n",
    "    model_name = 'resnext50_32x4d'\n",
    "    size = 512\n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 15\n",
    "    T_max = 6\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-6\n",
    "    batch_size = 4\n",
    "    weight_decay = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    target_size = 11\n",
    "    target_cols = ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n",
    "                   'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal',\n",
    "                   'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n",
    "                   'Swan Ganz Catheter Present']\n",
    "    n_fold = 4\n",
    "    trn_fold = [0]\n",
    "    train = True\n",
    "    train_path = str(TRAIN_PATH)\n",
    "    test_path = str(TEST_PATH)\n",
    "    output_dir = str(OUTPUT_DIR)\n",
    "    \n",
    "  \n",
    "    patience = 3\n",
    "    early_stopping = True\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['StudyInstanceUID'].values\n",
    "        self.labels = df[CFG.target_cols].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            file_name = self.file_names[idx]\n",
    "            file_path = Path(CFG.train_path) / f'{file_name}.jpg'  \n",
    "            \n",
    "            if not file_path.exists():\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                image = np.zeros((CFG.size, CFG.size, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                image = cv2.imread(str(file_path))\n",
    "                if image is None:\n",
    "                    print(f\"Failed to read image: {file_path}\")\n",
    "                    image = np.zeros((CFG.size, CFG.size, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            \n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return image, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {file_name} at index {idx}: {str(e)}\")\n",
    "            image = np.zeros((CFG.size, CFG.size, 3), dtype=np.uint8)\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return image, label\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['StudyInstanceUID'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = Path(CFG.test_path) / f'{file_name}.jpg'\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            print(f\"Test file not found: {file_path}\")\n",
    "            image = np.zeros((CFG.size, CFG.size, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.imread(str(file_path))\n",
    "            if image is None:\n",
    "                print(f\"Failed to read test image: {file_path}\")\n",
    "                image = np.zeros((CFG.size, CFG.size, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model Architecture\n",
    "# ====================================================\n",
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_name='resnext50_32x4d', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.fc.in_features\n",
    "        \n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, CFG.target_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(CFG.size, CFG.size, scale=(0.85, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(p=0.5, brightness_limit=0.2, contrast_limit=0.2),\n",
    "            ShiftScaleRotate(p=0.5, shift_limit=0.2, scale_limit=0.2, rotate_limit=20),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Metrics\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        score = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score, scores\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    predictions = (outputs.sigmoid() > 0.5).float()\n",
    "    correct = (predictions == targets).float().sum()\n",
    "    total = targets.numel()\n",
    "    return (correct / total).item()\n",
    "\n",
    "# ====================================================\n",
    "# Training Functions\n",
    "# ====================================================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc='Train')\n",
    "    for step, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_preds = model(images)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), \n",
    "            CFG.max_grad_norm\n",
    "        )\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "        \n",
    "        if (step + 1) % CFG.print_freq == 0:\n",
    "            pbar.set_postfix(**{\n",
    "                'train_loss': train_loss.avg,\n",
    "                'grad_norm': grad_norm.item(),\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            clear_memory()\n",
    "            \n",
    "    return train_loss.avg\n",
    "\n",
    "def valid_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = AverageMeter()\n",
    "    valid_acc = AverageMeter()  \n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc='Valid')\n",
    "    for step, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "         \n",
    "            acc = calculate_accuracy(y_preds, labels)\n",
    "            valid_acc.update(acc, batch_size)\n",
    "        \n",
    "        predictions.append(y_preds.sigmoid().cpu().numpy())\n",
    "        targets.append(labels.cpu().numpy())\n",
    "        \n",
    "        valid_loss.update(loss.item(), batch_size)\n",
    "        \n",
    "        if (step + 1) % CFG.print_freq == 0:\n",
    "            pbar.set_postfix({\n",
    "                'valid_loss': valid_loss.avg,\n",
    "                'valid_acc': valid_acc.avg\n",
    "            })\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    return valid_loss.avg, valid_acc.avg, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, valid_loader, model, criterion, optimizer, scheduler, fold):\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    \n",
    "  \n",
    "    log_file = f'{CFG.output_dir}/training_log_fold{fold}.csv'\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write('epoch,train_loss,train_acc,valid_loss,valid_acc,auc_score\\n')\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f'Epoch {epoch+1}/{CFG.epochs}')\n",
    "        \n",
    "    \n",
    "        model.train()\n",
    "        train_loss = AverageMeter()\n",
    "        train_acc = AverageMeter()\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc='Train'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = calculate_accuracy(outputs, labels)\n",
    "            train_loss.update(loss.item(), labels.size(0))\n",
    "            train_acc.update(acc, labels.size(0))\n",
    "        \n",
    "       \n",
    "        valid_loss, valid_acc, predictions, targets = valid_one_epoch(model, valid_loader, criterion, device)\n",
    "        score, scores = get_score(targets, predictions)\n",
    "        \n",
    "\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(valid_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "      \n",
    "        print(f'Train Loss: {train_loss.avg:.4f} Train Acc: {train_acc.avg:.4f}')\n",
    "        print(f'Valid Loss: {valid_loss:.4f} Valid Acc: {valid_acc:.4f}')\n",
    "        print(f'AUC Score: {score:.4f}')\n",
    "        \n",
    "      \n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f'{epoch+1},{train_loss.avg:.4f},{train_acc.avg:.4f},'\n",
    "                   f'{valid_loss:.4f},{valid_acc:.4f},{score:.4f}\\n')\n",
    "        \n",
    "    \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            print(f'Saving best model... Score: {best_score:.4f}')\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'predictions': predictions,\n",
    "                'targets': targets,\n",
    "                'epoch': epoch,\n",
    "                'score': best_score\n",
    "            }, f'{CFG.output_dir}/best_fold{fold}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "      \n",
    "        if CFG.early_stopping and patience_counter >= CFG.patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            break\n",
    "            \n",
    "        clear_memory()\n",
    "    \n",
    "    return best_score, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Main Training Function\n",
    "# ====================================================\n",
    "def train_model():\n",
    "    train_csv_path = INPUT_DIR / 'train.csv'\n",
    "    test_csv_path = INPUT_DIR / 'sample_submission_copy.csv'\n",
    "    \n",
    "    print(f\"Loading data from: {train_csv_path}\")\n",
    "    train = pd.read_csv(train_csv_path)\n",
    "    test = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    if CFG.debug:\n",
    "        train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "    groups = train['PatientID'].values\n",
    "    for n, (train_index, valid_index) in enumerate(Fold.split(train, train[CFG.target_cols], groups)):\n",
    "        train.loc[valid_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    \n",
    "    for fold in CFG.trn_fold:\n",
    "        print(f'Training fold {fold}')\n",
    "        \n",
    "        train_df = train[train.fold != fold].reset_index(drop=True)\n",
    "        valid_df = train[train.fold == fold].reset_index(drop=True)\n",
    "        \n",
    "        train_dataset = TrainDataset(train_df, transform=get_transforms(data='train'))\n",
    "        valid_dataset = TrainDataset(valid_df, transform=get_transforms(data='valid'))\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=CFG.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        model = CustomResNext(CFG.model_name, pretrained=True)\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        best_score, best_loss = train_loop(\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            fold\n",
    "        )\n",
    "        \n",
    "        print(f'Best Score: {best_score:.4f} Best Loss: {best_loss:.4f}')\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    print('Loading test data...')\n",
    "    test_csv_path = INPUT_DIR / 'sample_submission_copy.csv'\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "   \n",
    "    test_dataset = TestDataset(\n",
    "        test_df, \n",
    "        transform=get_transforms(data='valid')\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "  \n",
    "    models = []\n",
    "    for fold in CFG.trn_fold:\n",
    "        model = CustomResNext(CFG.model_name, pretrained=False)\n",
    "        model_path = f'{OUTPUT_DIR}/best_fold{fold}.pth'\n",
    "        model.load_state_dict(torch.load(model_path)['model'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader, desc='Predict'):\n",
    "            images = images.to(device)\n",
    "            outputs = None\n",
    "            \n",
    "        \n",
    "            for model in models:\n",
    "                if outputs is None:\n",
    "                    outputs = model(images).sigmoid()\n",
    "                else:\n",
    "                    outputs += model(images).sigmoid()\n",
    "            outputs /= len(models)\n",
    "            \n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    \n",
    "   \n",
    "    submission = pd.DataFrame(data=predictions, columns=CFG.target_cols)\n",
    "    submission['StudyInstanceUID'] = test_df['StudyInstanceUID']\n",
    "    submission.to_csv(INPUT_DIR / 'sample_submission_copy.csv', index=False)\n",
    "    print('Predictions saved to sample_submission_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Loading data from: /Users/sankalpkashyap/Desktop/UCDavisStudy/ECS271-MLD/Project/CatheterPositionViT/input/ranzcr-clip-catheter-line-classification/train.csv\n",
      "Training fold 0\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf00ab7bd1b4fcaa4dc1d32e87f9b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c31fdc296e41d7bc218e9a65ace941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3306 Train Acc: 0.8552\n",
      "Valid Loss: 0.2366 Valid Acc: 0.9013\n",
      "AUC Score: 0.7761\n",
      "Saving best model... Score: 0.7761\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdec83edb434f61afa6b23648c79379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78feb51c5e384b8f8ee396a236d671fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2427 Train Acc: 0.9000\n",
      "Valid Loss: 0.2218 Valid Acc: 0.9040\n",
      "AUC Score: 0.8122\n",
      "Saving best model... Score: 0.8122\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611e7e56ef2842d0ad06bbde87c72283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b3ccb371c24f389d46a9836d816185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2247 Train Acc: 0.9055\n",
      "Valid Loss: 0.2035 Valid Acc: 0.9089\n",
      "AUC Score: 0.8690\n",
      "Saving best model... Score: 0.8690\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ef395dbfeb4cef824291d50507b378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bb8f9b8654491bab8f5bb4b87b6c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2068 Train Acc: 0.9102\n",
      "Valid Loss: 0.1859 Valid Acc: 0.9167\n",
      "AUC Score: 0.8964\n",
      "Saving best model... Score: 0.8964\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49790c53f4846f79038c44856d62546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7599da35514ba2a13ca3f265bccaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1959 Train Acc: 0.9152\n",
      "Valid Loss: 0.1801 Valid Acc: 0.9202\n",
      "AUC Score: 0.9041\n",
      "Saving best model... Score: 0.9041\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5352da0755bf46e3924462042104f4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70219ed25b7b4c039f40f08afc71ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1875 Train Acc: 0.9185\n",
      "Valid Loss: 0.1778 Valid Acc: 0.9204\n",
      "AUC Score: 0.9085\n",
      "Saving best model... Score: 0.9085\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fda00ebfc1a4df19914ff2a66f47685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77994becb19341ab8fbddbd7ab3e7260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1863 Train Acc: 0.9204\n",
      "Valid Loss: 0.1756 Valid Acc: 0.9229\n",
      "AUC Score: 0.9083\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673305d3bfd74d2e80c2514a30201182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976b2a111d12451f8f30dfba2f8663b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1852 Train Acc: 0.9195\n",
      "Valid Loss: 0.1748 Valid Acc: 0.9219\n",
      "AUC Score: 0.9119\n",
      "Saving best model... Score: 0.9119\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf1ee6148e34e6db9ef1cc31650fd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f580d5356c47649e0d61aca64b6623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1870 Train Acc: 0.9197\n",
      "Valid Loss: 0.1735 Valid Acc: 0.9237\n",
      "AUC Score: 0.9103\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c001564a724b1c91e536861ab872ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37a0a87b6e64f8b89c4bef21659684f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1864 Train Acc: 0.9196\n",
      "Valid Loss: 0.1799 Valid Acc: 0.9199\n",
      "AUC Score: 0.9091\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abc00358c8f4fd9bdb7d940204c2d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/5640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd6f0c8a44e4897a1d064934e47180a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1892 Train Acc: 0.9196\n",
      "Valid Loss: 0.1842 Valid Acc: 0.9189\n",
      "AUC Score: 0.9086\n",
      "Early stopping triggered after 11 epochs\n",
      "Best Score: 0.9119 Best Loss: 0.1748\n",
      "Training completed!\n",
      "\n",
      "Generating test predictions...\n",
      "Loading test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00289c9f74b94d36aa1eee1bb7432f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict:   0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to sample_submission_copy.csv\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Training started...')\n",
    "    seed_everything(CFG.seed)\n",
    "    train_model()\n",
    "    print('Training completed!')\n",
    "    \n",
    "    print('\\nGenerating test predictions...')\n",
    "    predict_test()\n",
    "    print('All done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
